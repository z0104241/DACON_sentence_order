#!/usr/bin/env python3
"""
ê°€ë²¼ìš´ í›„ì²˜ë¦¬ ìŠ¤í¬ë¦½íŠ¸
ê°„ë‹¨í•˜ê³  ë¹ ë¥¸ ì‹œê°„ í‘œí˜„ ë¶„ì„
"""

import pandas as pd
import re
import argparse
from typing import List, Dict, Tuple, Union
import time
from collections import Counter

def extract_first_words_and_patterns(sentence: str) -> List[str]:
    """ë¬¸ì¥ì—ì„œ ì²« ë‹¨ì–´ë“¤ê³¼ ì‹œê°„ íŒ¨í„´ ì¶”ì¶œ"""
    
    # ì „ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
    clean_sentence = re.sub(r'[^\w\sê°€-í£]', ' ', sentence)
    words = clean_sentence.split()
    
    if not words:
        return []
    
    # ì²« 2-3ê°œ ë‹¨ì–´ ì¶”ì¶œ (ì‹œê°„ í‘œí˜„ì€ ë³´í†µ ë¬¸ì¥ ì•ë¶€ë¶„ì—)
    first_words = words[:3]
    
    # ì¶”ê°€ë¡œ ì „ì²´ ë¬¸ì¥ì—ì„œ ì‹œê°„ í‘œí˜„ íŒ¨í„´ ê²€ìƒ‰
    temporal_patterns = [
        r'ë¨¼ì €', r'ì²˜ìŒ', r'ìš°ì„ ', r'ì¼ë‹¨', r'ì‹œì‘',
        r'ê·¸ë‹¤ìŒ', r'ì´í›„', r'ë‹¤ìŒ', r'ê·¸ëŸ¬ê³ \s*ë‚˜ì„œ', r'ê³„ì†',
        r'ê·¸ë¦¬ê³ ', r'ë˜í•œ', r'í•œí¸', r'ë™ì‹œì—',
        r'ê·¸ë˜ì„œ', r'ë”°ë¼ì„œ', r'ê²°êµ­', r'ë§ˆì§€ë§‰', r'ìµœì¢…'
    ]
    
    found_patterns = []
    for pattern in temporal_patterns:
        if re.search(pattern, sentence):
            found_patterns.append(pattern.replace(r'\s*', ''))
    
    return first_words + found_patterns

def analyze_temporal_order_simple(sentences: List[str]) -> List[int]:
    """ê°„ë‹¨í•œ ì‹œê°„ ìˆœì„œ ë¶„ì„"""
    
    # ì‹œê°„ í‘œí˜„ë³„ ìš°ì„ ìˆœìœ„ (ë” í¬ê´„ì )
    time_priorities = {
        # ê°•í•œ ì‹œì‘ í‘œí˜„ (-3ì )
        'ë¨¼ì €': -3, 'ì²˜ìŒ': -3, 'ìš°ì„ ': -3, 'ì¼ë‹¨': -3, 'ì‹œì‘': -3,
        'ê°€ì¥ë¨¼ì €': -3, 'ì œì¼ë¨¼ì €': -3, 'ìš°ì„ ì ìœ¼ë¡œ': -3,
        
        # ì´ˆê¸° ë‹¨ê³„ (-2ì )  
        'ê·¸ë‹¤ìŒ': -2, 'ì´í›„': -2, 'ë‹¤ìŒ': -2, 'ê·¸í›„': -2, 'ê·¸ë’¤': -2,
        'ê³„ì†': -2, 'ì´ì–´ì„œ': -2, 'ê·¸ëŸ¬ê³ ë‚˜ì„œ': -2,
        
        # ì¤‘ê°„ ë‹¨ê³„ (-1ì )
        'ê·¸ë¦¬ê³ ': -1, 'ë˜í•œ': -1, 'í•œí¸': -1, 'ë™ì‹œì—': -1,
        'ê·¸ëŸ°ë°': -1, 'ê·¸ëŸ¬ë‚˜': -1,
        
        # ì¤‘ë¦½ (0ì )
        'ê·¸': 0, 'ì´': 0, 'ê·¸ë…€': 0, 'ê·¸ë“¤': 0,
        
        # í›„ê¸° ë‹¨ê³„ (+1ì )
        'ê·¸ë˜ì„œ': 1, 'ë”°ë¼ì„œ': 1, 'ê·¸ê²°ê³¼': 1, 'ì´ì—': 1,
        'ê·¸ëŸ¬ë¯€ë¡œ': 1, 'ê·¸ë¡œì¸í•´': 1,
        
        # ê°•í•œ ë§ˆì§€ë§‰ í‘œí˜„ (+2ì )
        'ë§ˆì§€ë§‰': 2, 'ê²°êµ­': 2, 'ìµœì¢…ì ìœ¼ë¡œ': 2, 'ëìœ¼ë¡œ': 2,
        'ë“œë””ì–´': 2, 'ë§ˆì¹¨ë‚´': 2, 'ìµœì¢…': 2, 'ê²°ë¡ ì ìœ¼ë¡œ': 2,
        
        # ë§¤ìš° ê°•í•œ ë§ˆì§€ë§‰ (+3ì )
        'ê°€ì¥ë§ˆì§€ë§‰': 3, 'ì œì¼ë§ˆì§€ë§‰': 3, 'ìµœì¢…ì ': 3
    }
    
    sentence_scores = []
    
    for i, sentence in enumerate(sentences):
        total_score = 0
        found_expressions = extract_first_words_and_patterns(sentence)
        
        # ê° í‘œí˜„ì˜ ì ìˆ˜ í•©ì‚°
        for expr in found_expressions:
            clean_expr = re.sub(r'\s+', '', expr.lower())
            if clean_expr in time_priorities:
                total_score += time_priorities[clean_expr]
        
        # ë¬¸ì¥ ê¸¸ì´ë„ ì•½ê°„ ê³ ë ¤ (ê¸´ ë¬¸ì¥ì€ ë³´í†µ ì¤‘ê°„ì´ë‚˜ ë§ˆì§€ë§‰)
        length_bonus = min(len(sentence) // 20, 1) * 0.1
        total_score += length_bonus
        
        sentence_scores.append((i, total_score))
    
    # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
    sorted_sentences = sorted(sentence_scores, key=lambda x: x[1])
    
    # ìˆœìœ„ í• ë‹¹ (ê°€ì¥ ë‚®ì€ ì ìˆ˜ê°€ 1ìˆœìœ„)
    priorities = [0] * 4
    for rank, (sent_idx, score) in enumerate(sorted_sentences):
        priorities[sent_idx] = rank + 1
    
    return priorities

def enhanced_pattern_matching(sentences: List[str]) -> str:
    """ê°•í™”ëœ íŒ¨í„´ ë§¤ì¹­ ê¸°ë°˜ í›„ì²˜ë¦¬"""
    
    # ë§¤ìš° ëª…í™•í•œ ìˆœì„œ ì§€ì‹œì–´ë“¤
    explicit_order_patterns = {
        r'ì²«\s*ë²ˆì§¸': 1, r'1ë²ˆì§¸': 1, r'ì²«ì§¸': 1,
        r'ë‘\s*ë²ˆì§¸': 2, r'2ë²ˆì§¸': 2, r'ë‘˜ì§¸': 2,  
        r'ì„¸\s*ë²ˆì§¸': 3, r'3ë²ˆì§¸': 3, r'ì…‹ì§¸': 3,
        r'ë„¤\s*ë²ˆì§¸': 4, r'4ë²ˆì§¸': 4, r'ë„·ì§¸': 4
    }
    
    sentence_positions = [-1] * 4
    
    # ëª…ì‹œì  ìˆœì„œ ì§€ì‹œì–´ ì°¾ê¸°
    for i, sentence in enumerate(sentences):
        for pattern, position in explicit_order_patterns.items():
            if re.search(pattern, sentence):
                sentence_positions[position-1] = i
                break
    
    # ëª…ì‹œì  ì§€ì‹œì–´ê°€ 2ê°œ ì´ìƒ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ
    explicit_count = sum(1 for pos in sentence_positions if pos != -1)
    if explicit_count >= 2:
        # ë¹ˆ ìë¦¬ëŠ” ì›ë˜ ìˆœì„œ ìœ ì§€
        result = []
        remaining = [i for i in range(4) if i not in sentence_positions]
        remaining_idx = 0
        
        for pos in sentence_positions:
            if pos != -1:
                result.append(str(pos))
            else:
                result.append(str(remaining[remaining_idx]))
                remaining_idx += 1
        
        return ''.join(result)
    
    return None  # ëª…ì‹œì  ì§€ì‹œì–´ê°€ ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ ë°©ë²• ì‚¬ìš©

def question_answer_pattern(sentences: List[str]) -> str:
    """ì§ˆë¬¸-ë‹µë³€ íŒ¨í„´ ì¸ì‹"""
    
    question_patterns = [r'\?', r'ê¹Œ\?', r'ë‚˜\?', r'ëŠ”ê°€\?', r'ì¼ê¹Œ\?', r'ë¬´ì—‡', r'ì–´ë–»ê²Œ', r'ì™œ', r'ì–¸ì œ']
    answer_patterns = [r'ë‹µ:', r'ë‹µì€', r'ëŒ€ë‹µ', r'~ë‹¤\.', r'~ì´ë‹¤\.', r'~í•œë‹¤\.']
    
    question_sentences = []
    answer_sentences = []
    
    for i, sentence in enumerate(sentences):
        # ì§ˆë¬¸ íŒ¨í„´ ì²´í¬
        for pattern in question_patterns:
            if re.search(pattern, sentence):
                question_sentences.append(i)
                break
        
        # ë‹µë³€ íŒ¨í„´ ì²´í¬  
        for pattern in answer_patterns:
            if re.search(pattern, sentence):
                answer_sentences.append(i)
                break
    
    # ì§ˆë¬¸ì´ ìˆê³  ë‹µë³€ì´ ìˆìœ¼ë©´ ì§ˆë¬¸ì„ ë‹µë³€ ì•ìœ¼ë¡œ
    if len(question_sentences) == 1 and len(answer_sentences) >= 1:
        q_idx = question_sentences[0]
        a_idx = answer_sentences[0]
        
        # ê°„ë‹¨í•œ ì¬ë°°ì—´: ì§ˆë¬¸ â†’ ë‹µë³€ ìˆœì„œë¡œ
        remaining = [i for i in range(4) if i not in [q_idx, a_idx]]
        return f"{q_idx}{a_idx}{remaining[0]}{remaining[1]}"
    
    return None

def lightweight_postprocessing(predicted_order: str, sentences: List[str]) -> str:
    """ê°€ë²¼ìš´ í›„ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
    
    # 1ìˆœìœ„: ëª…ì‹œì  ìˆœì„œ ì§€ì‹œì–´
    explicit_result = enhanced_pattern_matching(sentences)
    if explicit_result:
        return explicit_result
    
    # 2ìˆœìœ„: ì§ˆë¬¸-ë‹µë³€ íŒ¨í„´
    qa_result = question_answer_pattern(sentences)
    if qa_result:
        return qa_result
    
    # 3ìˆœìœ„: ì‹œê°„ í‘œí˜„ ë¶„ì„
    try:
        priorities = analyze_temporal_order_simple(sentences)
        
        # ìš°ì„ ìˆœìœ„ê°€ ëª…í™•íˆ ë‹¤ë¥¸ ê²½ìš°ë§Œ ì ìš©
        unique_priorities = len(set(priorities))
        if unique_priorities >= 3:
            # ìš°ì„ ìˆœìœ„ë¥¼ ìˆœì„œë¡œ ë³€í™˜
            order_pairs = [(i, priorities[i]) for i in range(4)]
            sorted_pairs = sorted(order_pairs, key=lambda x: x[1])
            new_order = ''.join(str(pair[0]) for pair in sorted_pairs)
            return new_order
    except:
        pass
    
    # ëª¨ë“  ë°©ë²•ì´ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ìœ ì§€
    return predicted_order

def apply_lightweight_postprocessing(input_file: str, output_file: str) -> None:
    """ê°€ë²¼ìš´ í›„ì²˜ë¦¬ ì ìš©"""
    
    print(f"ğŸ“‚ AI ì¶”ë¡  ê²°ê³¼ ë¡œë“œ: {input_file}")
    df = pd.read_csv(input_file)
    
    postprocessed_results = []
    changes_count = 0
    method_stats = Counter()
    
    print("âš¡ ê°€ë²¼ìš´ í›„ì²˜ë¦¬ ì ìš© ì¤‘...")
    
    for idx, row in df.iterrows():
        if row['parsing_status'] == 'SUCCESS':
            original_order = f"{row['answer_0']}{row['answer_1']}{row['answer_2']}{row['answer_3']}"
            sentences = [row['sentence_0'], row['sentence_1'], row['sentence_2'], row['sentence_3']]
            
            # ê° ë°©ë²•ë³„ë¡œ ì‹œë„í•´ì„œ ì–´ë–¤ ë°©ë²•ì´ íš¨ê³¼ì ì¸ì§€ ì¶”ì 
            explicit_result = enhanced_pattern_matching(sentences)
            qa_result = question_answer_pattern(sentences)
            
            # ìµœì¢… í›„ì²˜ë¦¬ ì ìš©
            postprocessed_order = lightweight_postprocessing(original_order, sentences)
            
            # ë³€ê²½ ì›ì¸ ì¶”ì 
            if postprocessed_order != original_order:
                changes_count += 1
                if explicit_result and explicit_result == postprocessed_order:
                    method_stats['explicit_order'] += 1
                elif qa_result and qa_result == postprocessed_order:
                    method_stats['question_answer'] += 1
                else:
                    method_stats['temporal_analysis'] += 1
                
                print(f"ğŸ”„ ë³€ê²½ {row['ID']}: {original_order} â†’ {postprocessed_order}")
            
            # ê²°ê³¼ ì €ì¥
            postprocessed_results.append({
                'ID': row['ID'],
                'answer_0': int(postprocessed_order[0]),
                'answer_1': int(postprocessed_order[1]),
                'answer_2': int(postprocessed_order[2]),
                'answer_3': int(postprocessed_order[3]),
                'raw_output': '',
                'parsing_status': 'SUCCESS'
            })
        else:
            # íŒŒì‹± ì‹¤íŒ¨ ì¼€ì´ìŠ¤ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
            postprocessed_results.append({
                'ID': row['ID'],
                'answer_0': row['answer_0'],
                'answer_1': row['answer_1'],
                'answer_2': row['answer_2'],
                'answer_3': row['answer_3'],
                'raw_output': row['raw_output'],
                'parsing_status': row['parsing_status']
            })
    
    # ê²°ê³¼ ì €ì¥
    postprocessed_df = pd.DataFrame(postprocessed_results)
    postprocessed_df.to_csv(output_file, index=False, encoding='utf-8-sig')
    
    # í†µê³„ ì¶œë ¥
    print(f"ğŸ’¾ ê°€ë²¼ìš´ í›„ì²˜ë¦¬ ê²°ê³¼ ì €ì¥: {output_file}")
    print(f"ğŸ”„ ì´ ë³€ê²½ëœ ì¼€ì´ìŠ¤: {changes_count}ê°œ")
    print(f"ğŸ“Š ë³€ê²½ ë¹„ìœ¨: {(changes_count/len(df)*100):.1f}%")
    
    print(f"\nğŸ“ˆ ë°©ë²•ë³„ íš¨ê³¼:")
    for method, count in method_stats.items():
        print(f"   {method}: {count}ê°œ")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ê°€ë²¼ìš´ í›„ì²˜ë¦¬ (ë¹ ë¥´ê³  íš¨ìœ¨ì )')
    parser.add_argument('--input', '-i', default='ai_predictions.csv', help='AI ì¶”ë¡  ê²°ê³¼ CSV íŒŒì¼')
    parser.add_argument('--output', '-o', default='lightweight_postprocessed.csv', help='í›„ì²˜ë¦¬ ê²°ê³¼ CSV íŒŒì¼')
    
    args = parser.parse_args()
    
    try:
        start_time = time.time()
        
        # ê°€ë²¼ìš´ í›„ì²˜ë¦¬ ì ìš©
        apply_lightweight_postprocessing(args.input, args.output)
        
        elapsed_time = time.time() - start_time
        print(f"\nâ±ï¸  ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.1f}ì´ˆ (ë§¤ìš° ë¹ ë¦„!)")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")

if __name__ == "__main__":
    main()