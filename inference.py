#!/usr/bin/env python3
"""
ìˆœìˆ˜ AI ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ (í›„ì²˜ë¦¬ ì œì™¸)
Few-shot í”„ë¡¬í”„íŠ¸ + ë°°ì¹˜ ì²˜ë¦¬ + ë©”ëª¨ë¦¬ ê´€ë¦¬
"""

import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import re
from tqdm import tqdm
import argparse
import os
from typing import List, Tuple, Dict, Any, Union
import time
from config import (
    MODEL_NAME, 
    HUGGINGFACE_REPO, 
    ADAPTER_SUBFOLDER, 
    CACHE_DIR,
    USE_4BIT,
    BNB_4BIT_COMPUTE_DTYPE,
    BNB_4BIT_QUANT_TYPE,
    BNB_4BIT_USE_DOUBLE_QUANT,
    MAX_NEW_TOKENS,
    INFERENCE_BATCH_SIZE,
    PROMPT_TEMPLATE,
    OUTPUT_DIR  
)


### í—ˆê¹…í˜ì´ìŠ¤ ì—…ë¡œë“œìš©
# def load_model_and_tokenizer() -> Tuple[PeftModel, AutoTokenizer]: 
#     """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ"""
#     print("ğŸ”§ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    
#     # í† í¬ë‚˜ì´ì €
#     tokenizer = AutoTokenizer.from_pretrained(
#         MODEL_NAME,  
#         trust_remote_code=True,
#         cache_dir=CACHE_DIR  
#     )
#     tokenizer.pad_token = tokenizer.eos_token
#     tokenizer.padding_side = "left"
    
#     # ì–‘ìí™” ì„¤ì •
#     compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)
    
#     bnb_config = BitsAndBytesConfig(
#         load_in_4bit=USE_4BIT,
#         bnb_4bit_use_double_quant=BNB_4BIT_USE_DOUBLE_QUANT,
#         bnb_4bit_quant_type=BNB_4BIT_QUANT_TYPE,
#         bnb_4bit_compute_dtype=compute_dtype,
#     )
    
#     # ë² ì´ìŠ¤ ëª¨ë¸
#     base_model = AutoModelForCausalLM.from_pretrained(
#         MODEL_NAME,
#         quantization_config=bnb_config,
#         device_map="auto",
#         trust_remote_code=True,
#         torch_dtype=torch.float16,
#         cache_dir=CACHE_DIR
#     )
    
#     # LoRA ì–´ëŒ‘í„° ë¡œë“œ
#     model = PeftModel.from_pretrained(
#         base_model, 
#         HUGGINGFACE_REPO,
#         subfolder=ADAPTER_SUBFOLDER,
#         cache_dir=CACHE_DIR
#     )
#     model.eval()
    
#     print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
#     return model, tokenizer



def load_model_and_tokenizer() -> Tuple[PeftModel, AutoTokenizer]:
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ (ë¡œì»¬ ì–´ëŒ‘í„° ê²½ë¡œ ì‚¬ìš©)"""
    print("ğŸ”§ ëª¨ë¸ ë¡œë“œ ì¤‘...")

    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,  
        trust_remote_code=True,
        cache_dir=CACHE_DIR  
    )
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "left"

    compute_dtype = getattr(torch, BNB_4BIT_COMPUTE_DTYPE)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=USE_4BIT,
        bnb_4bit_use_double_quant=BNB_4BIT_USE_DOUBLE_QUANT,
        bnb_4bit_quant_type=BNB_4BIT_QUANT_TYPE,
        bnb_4bit_compute_dtype=compute_dtype,
    )
    base_model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
        cache_dir=CACHE_DIR
    )

    # === ë¡œì»¬ ì–´ëŒ‘í„° ë¡œë“œ ===
    
    model = PeftModel.from_pretrained(
        base_model, 
        OUTPUT_DIR    
    )
    model.eval()

    print("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
    return model, tokenizer

def create_fewshot_prompt(sentences):
    """config.pyì˜ í…œí”Œë¦¿ìœ¼ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    return PROMPT_TEMPLATE.format(
        sentence_0=sentences[0],
        sentence_1=sentences[1],
        sentence_2=sentences[2],
        sentence_3=sentences[3]
    )


def predict_batch(sentences_batch: List[List[str]], model: PeftModel, tokenizer: AutoTokenizer) -> List[str]:
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¬¸ì¥ ìˆœì„œ ì˜ˆì¸¡"""
    
    # í”„ë¡¬í”„íŠ¸ ìƒì„± (Few-shot ì ìš©)
    prompts = [create_fewshot_prompt(sentences) for sentences in sentences_batch]
    
    # ë©”ì‹œì§€ í˜•íƒœë¡œ ë³€í™˜
    messages_batch = [[{"role": "user", "content": prompt}] for prompt in prompts]
    texts = [tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) 
             for messages in messages_batch]
    
    # í† í¬ë‚˜ì´ì§•
    inputs = tokenizer(
        texts, 
        return_tensors="pt", 
        padding=True, 
        truncation=True, 
        max_length=1024
    ).to(model.device)
    
    # ë°°ì¹˜ ì¶”ë¡  (greedy decoding)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=False,  # greedy decoding
            temperature=None,
            top_p=None,
            top_k=None,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.05
        )
    
    # ê²°ê³¼ ë””ì½”ë”©
    results = []
    for i, output in enumerate(outputs):
        input_length = inputs.input_ids[i].shape[0]
        generated = tokenizer.decode(output[input_length:], skip_special_tokens=True).strip()
        results.append(extract_order_enhanced(generated))
    
    return results

def extract_order_enhanced(text: str) -> str:
    """ê°•í™”ëœ íŒŒì‹± í•¨ìˆ˜"""
    
    # 1ìˆœìœ„: ì •í™•í•œ íŒ¨í„´ë“¤
    exact_patterns = [
        r'ë‹µ[:ï¼š]\s*([0-3]),\s*([0-3]),\s*([0-3]),\s*([0-3])',
        r'ë‹µ[:ï¼š]\s*([0-3])\s*,\s*([0-3])\s*,\s*([0-3])\s*,\s*([0-3])',
        r'ìˆœì„œ[:ï¼š]\s*([0-3]),\s*([0-3]),\s*([0-3]),\s*([0-3])',
        r'([0-3]),\s*([0-3]),\s*([0-3]),\s*([0-3])',
        r'([0-3])\s*â†’\s*([0-3])\s*â†’\s*([0-3])\s*â†’\s*([0-3])',
        r'([0-3])\s+([0-3])\s+([0-3])\s+([0-3])',
        r'([0-3])-([0-3])-([0-3])-([0-3])',
        r'([0-3])\.([0-3])\.([0-3])\.([0-3])',
    ]
    
    for pattern in exact_patterns:
        match = re.search(pattern, text)
        if match:
            if len(match.groups()) == 4:
                result = [int(g) for g in match.groups()]
            else:
                result = [int(d) for d in match.group(1) if d.isdigit()]
                
            if len(result) == 4 and set(result) == {0,1,2,3}:
                return ''.join(map(str, result))
    
    # 2ìˆœìœ„: 4ìë¦¬ ì—°ì† ìˆ«ì
    four_digit = re.search(r'\b([0-3]{4})\b', text)
    if four_digit:
        return four_digit.group(1)
    
    # 3ìˆœìœ„: ìˆœì„œ/ë‹µ í‚¤ì›Œë“œ ë’¤ ìˆ«ìë“¤
    patterns = [r'ìˆœì„œ[:ï¼š]\s*([0-3\s,]+)', r'ë‹µ[:ï¼š]\s*([0-3\s,]+)', r'ê²°ê³¼[:ï¼š]\s*([0-3\s,]+)']
    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            numbers = re.findall(r'[0-3]', match.group(1))
            if len(numbers) >= 4:
                return ''.join(numbers[:4])
    
    # 4ìˆœìœ„: ë§ˆì§€ë§‰ ì¤„ì—ì„œ ìˆ«ìë“¤
    lines = [line.strip() for line in text.split('\n') if line.strip()]
    if lines:
        numbers = re.findall(r'[0-3]', lines[-1])
        if len(numbers) >= 4:
            return ''.join(numbers[:4])
    
    # 5ìˆœìœ„: ì „ì²´ì—ì„œ ìˆ«ìë“¤
    all_numbers = re.findall(r'[0-3]', text)
    if len(all_numbers) >= 4:
        return ''.join(all_numbers[-4:])
    
    # íŒŒì‹± ì‹¤íŒ¨ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜
    return text

def extract_sentences(row: pd.Series) -> List[str]:
    """í–‰ì—ì„œ ë¬¸ì¥ë“¤ ì¶”ì¶œ"""
    patterns = [
        ['sentence_0', 'sentence_1', 'sentence_2', 'sentence_3'],
        ['sent_0', 'sent_1', 'sent_2', 'sent_3'],
        ['text_0', 'text_1', 'text_2', 'text_3'],
        ['0', '1', '2', '3']
    ]
    
    for pattern in patterns:
        try:
            return [str(row[col]) for col in pattern]
        except KeyError:
            continue
    
    # ì²˜ìŒ 4ê°œ ì»¬ëŸ¼ ì‚¬ìš©
    return [str(row.iloc[i]) for i in range(min(4, len(row)))]

def process_result(predicted_order: str) -> Dict[str, Union[int, str]]:
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì²˜ë¦¬ (í›„ì²˜ë¦¬ ì—†ìŒ)"""
    if len(predicted_order) == 4 and predicted_order.isdigit():
        # íŒŒì‹± ì„±ê³µ
        return {
            'answer_0': int(predicted_order[0]),
            'answer_1': int(predicted_order[1]),
            'answer_2': int(predicted_order[2]),
            'answer_3': int(predicted_order[3]),
            'raw_output': '',
            'parsing_status': 'SUCCESS'
        }
    else:
        # íŒŒì‹± ì‹¤íŒ¨
        return {
            'answer_0': '',
            'answer_1': '',
            'answer_2': '',
            'answer_3': '',
            'raw_output': predicted_order,
            'parsing_status': 'FAILED'
        }

def main(input_file: str, output_file: str) -> pd.DataFrame:
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    start_time = time.time()
    
    # ëª¨ë¸ ë¡œë“œ
    model, tokenizer = load_model_and_tokenizer()
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(input_file)#.head(5)
    print(f"ğŸ“‚ ë°ì´í„° ë¡œë“œ: {len(df)}ê°œ í–‰")
    
    # ë°°ì¹˜ë³„ ì²˜ë¦¬
    results = []
    total_batches = (len(df) + INFERENCE_BATCH_SIZE - 1) // INFERENCE_BATCH_SIZE
    
    print(f"ğŸš€ ë°°ì¹˜ í¬ê¸°: {INFERENCE_BATCH_SIZE}, ì´ ë°°ì¹˜: {total_batches}")
    
    for batch_idx in tqdm(range(total_batches), desc="ë°°ì¹˜ ì²˜ë¦¬"):
        # ë°°ì¹˜ ë°ì´í„° ì¤€ë¹„
        start_idx = batch_idx * INFERENCE_BATCH_SIZE
        end_idx = min(start_idx + INFERENCE_BATCH_SIZE, len(df))
        batch_rows = df.iloc[start_idx:end_idx]
        
        try:
            # ë°°ì¹˜ ë‚´ ë¬¸ì¥ë“¤ ì¶”ì¶œ
            sentences_batch = [extract_sentences(row) for _, row in batch_rows.iterrows()]
            
            # ë°°ì¹˜ ì˜ˆì¸¡
            predicted_orders = predict_batch(sentences_batch, model, tokenizer)
            
            # ê²°ê³¼ ì €ì¥ (ìˆœìˆ˜ AI ê²°ê³¼ë§Œ)
            for i, (row_idx, _) in enumerate(batch_rows.iterrows()):
                result = process_result(predicted_orders[i])
                results.append({
                    'ID': f'TEST_{row_idx:04d}',
                    'answer_0': result['answer_0'],
                    'answer_1': result['answer_1'],
                    'answer_2': result['answer_2'],
                    'answer_3': result['answer_3'],
                    'raw_output': result['raw_output'],
                    'parsing_status': result['parsing_status'],
                    # í›„ì²˜ë¦¬ìš© ì›ë³¸ ë¬¸ì¥ë“¤ ì €ì¥
                    'sentence_0': sentences_batch[i][0],
                    'sentence_1': sentences_batch[i][1],
                    'sentence_2': sentences_batch[i][2],
                    'sentence_3': sentences_batch[i][3]
                })
            
            # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ {batch_idx} ì˜¤ë¥˜: {e}")
            # ë°°ì¹˜ ì „ì²´ë¥¼ ì—ëŸ¬ë¡œ ì²˜ë¦¬
            for i, (row_idx, _) in enumerate(batch_rows.iterrows()):
                results.append({
                    'ID': f'TEST_{row_idx:04d}',
                    'answer_0': '',
                    'answer_1': '',
                    'answer_2': '',
                    'answer_3': '',
                    'raw_output': f'BATCH_ERROR: {e}',
                    'parsing_status': 'ERROR',
                    'sentence_0': '',
                    'sentence_1': '',
                    'sentence_2': '',
                    'sentence_3': ''
                })
    
    # ìµœì¢… ê²°ê³¼ ì €ì¥
    results_df = pd.DataFrame(results)
    results_df.iloc[:,:5].to_csv(output_file, index=False, encoding='utf-8-sig')
    
    # íŒŒì‹± ì„±ê³µë¥  ê³„ì‚°
    success_count = len(results_df[results_df['parsing_status'] == 'SUCCESS'])
    failed_count = len(results_df[results_df['parsing_status'] == 'FAILED'])
    error_count = len(results_df[results_df['parsing_status'] == 'ERROR'])
    total_count = len(results_df)
    
    success_rate = (success_count / total_count) * 100 if total_count > 0 else 0
    
    # ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
    elapsed_time = time.time() - start_time
    samples_per_sec = len(df) / elapsed_time
    
    print(f"ğŸ’¾ ìˆœìˆ˜ AI ì¶”ë¡  ê²°ê³¼ ì €ì¥: {output_file}")
    print(f"ğŸ“Š íŒŒì‹± ì„±ê³µë¥ : {success_rate:.1f}% ({success_count}/{total_count})")
    print(f"   - ì„±ê³µ: {success_count}")
    print(f"   - ì‹¤íŒ¨: {failed_count}")
    print(f"   - ì—ëŸ¬: {error_count}")
    print(f"â±ï¸  ì²˜ë¦¬ ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
    print(f"ğŸš€ ì²˜ë¦¬ ì†ë„: {samples_per_sec:.1f} ìƒ˜í”Œ/ì´ˆ")
    
    return results_df

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='ìˆœìˆ˜ AI ì¶”ë¡  (í›„ì²˜ë¦¬ ì œì™¸)')
    parser.add_argument('--input', '-i', default='test.csv', help='ì…ë ¥ CSV íŒŒì¼')
    parser.add_argument('--output', '-o', default='predictions_0527.csv', help='ì¶œë ¥ CSV íŒŒì¼')
    
    args = parser.parse_args()
    
    try:
        main(args.input, args.output)
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜: {e}")